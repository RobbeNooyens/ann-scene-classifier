# Self-Supervised Scene Classification Project

## Overview
This project explores training a scene classification model using both fully-supervised and self-supervised learning techniques. The primary aim is to compare these methods and assess the effectiveness of self-supervised learning in reducing the reliance on annotated data.

## Project Structure

- **main.py**: Entry point for training and evaluating the models.
- **models.py**: Contains model definitions and functions for loading, training, and evaluating the models.
- **data.py**: Handles data loading and processing, including applying transformations.
- **transformations.py**: Defines image transformations used in self-supervised tasks, such as Gaussian blur and perturbations.
- **logger.py**: Manages logging of accuracy and loss data, exporting it to CSV files.
- **config.py**: Contains configuration settings for different training setups, facilitating experimentation without code duplication.

## Datasets
The project uses the 15-Scene dataset, which includes 15 categories of scenes. The dataset is split into training, validation, and test sets.

## Training Schemes
### Fully-Supervised Learning
- **Model**: EfficientNet-B0 pre-trained on ImageNet.
- **Training**: Fine-tuned on the annotated 15-Scene dataset.
- **Parameters**:
  - Optimizer: Adam
  - Learning Rate: 0.00002 to 0.0001
  - Batch Size: 16
  - Dropout: 0.3 to 0.5

### Self-Supervised Learning
#### Gaussian Blur Pretext Task
- **Task**: Classify Gaussian blur kernel sizes (5x5, 9x9, 13x13, 17x17, 21x21).
- **Training**: Replace classifier with one that predicts blur sizes, fine-tune on scene classification.
- **Parameters**:
  - Learning Rate: 0.0001
  - Dropout: 0.2
  - Batch Size: 32

#### Perturbation Pretext Task
- **Task**: Classify black and white perturbations.
- **Training**: Replace classifier with one that predicts perturbation type, fine-tune on scene classification.
- **Parameters**:
  - Learning Rate: 0.00001 to 0.0001
  - Dropout: 0.2 to 0.5
  - Batch Size: 32

## Evaluation
Models are evaluated based on their classification accuracy on the scene classification task. Key metrics include:
- **Fully-Supervised Model**: Achieved a test accuracy of 94.64%.
- **Self-Supervised Gaussian Blur Model**: Achieved a test accuracy of 77.90%.
- **Self-Supervised Perturbation Model**: Achieved a test accuracy of 80.35%.

## Usage
1. **Setup**: Ensure all dependencies are installed.
2. **Configuration**: Adjust settings in `config.py` as needed.
3. **Run Training**: Execute `main.py` to start the training process.
4. **Monitor Performance**: Check logs generated by `logger.py` for accuracy and loss metrics.

## Conclusion
The results indicate that while self-supervised learning schemes can reduce the need for annotated data, the fully-supervised model still achieved the highest accuracy. The choice and optimization of pretext tasks are crucial for maximizing performance in downstream tasks.

---

This README provides an overview of the project, details on the structure and training schemes, and instructions on how to use the provided code. For detailed experimental results and analysis, refer to the final report `ANN_Final.pdf`.
